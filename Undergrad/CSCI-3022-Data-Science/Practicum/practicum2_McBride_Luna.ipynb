{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3022: Intro to Data Science - Summer 2019 Practicum 2\n",
    "***\n",
    "\n",
    "This practicum is due on Canvas by **11:59 PM on Thursday July 25**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "1. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distribution function or how to perform a particular operation in Python/Pandas. \n",
    "1. This is meant to be like a coding portion of your midterm exam. So, the instructional team will be much less helpful than we typically are with homework. For example, we will not check answers, help debug your code, and so on.\n",
    "1. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "2. You may **NOT** post to message boards or other online resources asking for help.\n",
    "3. You may **NOT** copy-paste solutions *from anywhere*.\n",
    "4. You may **NOT** collaborate with classmates or anyone else.\n",
    "5. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on this practicum or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**Name**:  Luna McBride\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the practicums nor can you drop your practicum grades. \n",
    "- If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a Practicum clarifications thread. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Bottom](#bot)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import whatever packages you think you will need here!\n",
    "import numpy as np #For the use of arrays for holding and comparisons\n",
    "import pandas as pd #For the dataframes that hold the data\n",
    "from scipy import stats #For the f statistic\n",
    "import math #For functions such as exp() ie e\n",
    "import statsmodels.api as sm  #For the models that give us all the betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "<a id='p1'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "### [70 points] Problem 1: Multiple Linear Regression to Explain House Hauntings\n",
    "\n",
    "<img src=\"https://s-media-cache-ak0.pinimg.com/originals/09/72/01/09720128cff5de4d4af038cd3fcf7f69.jpg\" style=\"width: 300px;\"/>\n",
    "\n",
    "In an effort to control the skyrocketing prices of real estate in the Colorado Front Range, Governor Polis implemented a cutting edge new intervention. This new program oversaw the introduction of ghosts back into their natural ecosystem, after the ghost population seriously dwindled in recent decades due to overhaunting. However, an unfortunate miscalculation has led to haunted houses becoming a very serious problem in Colorado. Modern problems require modern solutions, so Governor Polis has hired you and the famous hedgehog data scientist/part-time ghostbuster Amy to determine what features of a house may be used to best predict a `haunted` score, related to the probability that a house with the given features is haunted (higher $\\leftrightarrow$ more likely to be haunted).\n",
    "\n",
    "You decide to use multiple linear regression to understand and predict what factors lead to increased haunted house hazard. You collected a data set from Haunted Zillow, the lesser-known database of haunted house prices and attributes. The data cover a variety of potential features, and you'll find this data in the file `houses.csv`. \n",
    "\n",
    "**Response**: \n",
    "\n",
    "- $\\texttt{haunted}$: a haunting score, related to the probability that a house with the given features is haunted (higher $\\leftrightarrow$ more likely to be haunted)\n",
    "\n",
    "**Features**: \n",
    "\n",
    "- $\\texttt{age}$: age of the house, in years\n",
    "- $\\texttt{area}$: square footage of interior of house\n",
    "- $\\texttt{bathrooms}$: number of bathrooms\n",
    "- $\\texttt{distance metro}$: distance to the nearest major metropolitan area (in miles)\n",
    "- $\\texttt{distance cemetery}$: distance to the nearest cemetery (in miles)\n",
    "- $\\texttt{cats}$: the number of cats within a one-block radius of the house\n",
    "- $\\texttt{howls}$: the number of wolf howls heard on an average night in the house's neighborhood\n",
    "- $\\texttt{clouds}$: what percentage of the sky was covered by clouds (fraction, 0-1)\n",
    "- $\\texttt{precipitation}$: amount of precipitation in the past 72 hours (inches)\n",
    "- $\\texttt{misery index}$: an economic indicator for how miserable the average United States citizen is, based on the unemployment rate and the inflation rate. More [here](https://www.stuffyoushouldknow.com/podcasts/whats-the-misery-index.htm) and [here](https://en.wikipedia.org/wiki/Misery_index_(economics)). Higher values correspond to more miserable citizens.\n",
    "- $\\texttt{ice cream sold}$: the number of units of ice cream sold at the farmer's market the week the house was most recently sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Read the data from `houses.csv` into a Pandas DataFrame.  Note that since we will be doing a multiple linear regression we will need all of the features, so you should drop any row in the DataFrame that is missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>distance metro</th>\n",
       "      <th>distance cemetery</th>\n",
       "      <th>cats</th>\n",
       "      <th>howls</th>\n",
       "      <th>clouds</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>misery index</th>\n",
       "      <th>ice cream sold</th>\n",
       "      <th>haunted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.06</td>\n",
       "      <td>2041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>10.01</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>12.99</td>\n",
       "      <td>273</td>\n",
       "      <td>-0.596150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141.48</td>\n",
       "      <td>1564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.07</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>16.77</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.146465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.58</td>\n",
       "      <td>1637</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>16.49</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.303117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.47</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.43</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.92</td>\n",
       "      <td>8.28</td>\n",
       "      <td>146</td>\n",
       "      <td>0.339912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259.44</td>\n",
       "      <td>1642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>5.90</td>\n",
       "      <td>178</td>\n",
       "      <td>0.724867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>287.93</td>\n",
       "      <td>2676</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>4.57</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.37</td>\n",
       "      <td>8.59</td>\n",
       "      <td>186</td>\n",
       "      <td>0.805338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.92</td>\n",
       "      <td>2494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>16.55</td>\n",
       "      <td>259</td>\n",
       "      <td>-0.341216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>184.49</td>\n",
       "      <td>1157</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.13</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.07</td>\n",
       "      <td>105</td>\n",
       "      <td>0.182997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42.52</td>\n",
       "      <td>2520</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.79</td>\n",
       "      <td>317</td>\n",
       "      <td>0.301654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150.29</td>\n",
       "      <td>1790</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>7.32</td>\n",
       "      <td>285</td>\n",
       "      <td>0.152310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143.29</td>\n",
       "      <td>2389</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.49</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.58</td>\n",
       "      <td>16.60</td>\n",
       "      <td>178</td>\n",
       "      <td>-0.045025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>41.97</td>\n",
       "      <td>2019</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.04</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15.48</td>\n",
       "      <td>287</td>\n",
       "      <td>0.039757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>262.91</td>\n",
       "      <td>1985</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.88</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.32</td>\n",
       "      <td>5.77</td>\n",
       "      <td>263</td>\n",
       "      <td>-0.073846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.31</td>\n",
       "      <td>2717</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.41</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.76</td>\n",
       "      <td>14.29</td>\n",
       "      <td>213</td>\n",
       "      <td>-0.483913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>296.44</td>\n",
       "      <td>1943</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.81</td>\n",
       "      <td>19.58</td>\n",
       "      <td>182</td>\n",
       "      <td>0.326364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150.24</td>\n",
       "      <td>2002</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.16</td>\n",
       "      <td>5.29</td>\n",
       "      <td>96</td>\n",
       "      <td>0.143042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  area  bathrooms  distance metro  distance cemetery  cats  howls  \\\n",
       "0    65.06  2041        1.0             7.1              10.01     7      3   \n",
       "1   141.48  1564        0.0             7.4               4.07     5      5   \n",
       "2     7.58  1637        3.0             7.0               3.36     2      0   \n",
       "3    51.47  2021        2.0             7.9               3.43     6      8   \n",
       "4   259.44  1642        1.0             7.5               3.19     4      1   \n",
       "5   287.93  2676        2.0             8.1               4.57     7      6   \n",
       "6    86.92  2494        0.0             7.1               4.69     4      4   \n",
       "7   184.49  1157        3.0             6.8               3.13     6      4   \n",
       "8    42.52  2520        1.0             6.1               3.42     3      3   \n",
       "9   150.29  1790        1.0             5.6               4.44     4      9   \n",
       "10  143.29  2389        2.0             8.1               5.49     2      2   \n",
       "11   41.97  2019        4.0             7.4               6.04     4      7   \n",
       "12  262.91  1985        2.0             5.5               6.88     7      7   \n",
       "13    7.31  2717        3.0             5.9               5.41     3      5   \n",
       "14  296.44  1943        2.0             6.3               3.65     3      4   \n",
       "15  150.24  2002        5.0             6.6               3.45     4      7   \n",
       "\n",
       "    clouds  precipitation  misery index  ice cream sold   haunted  \n",
       "0     1.00           0.82         12.99             273 -0.596150  \n",
       "1     1.00           0.99         16.77             184 -0.146465  \n",
       "2     1.00           1.17         16.49             141 -0.303117  \n",
       "3     0.13           0.92          8.28             146  0.339912  \n",
       "4     1.00           1.73          5.90             178  0.724867  \n",
       "5     0.89           1.37          8.59             186  0.805338  \n",
       "6     1.00           0.90         16.55             259 -0.341216  \n",
       "7     0.40           0.88          6.07             105  0.182997  \n",
       "8     1.00           0.90          6.79             317  0.301654  \n",
       "9     1.00           0.91          7.32             285  0.152310  \n",
       "10    0.79           1.58         16.60             178 -0.045025  \n",
       "11    1.00           1.56         15.48             287  0.039757  \n",
       "12    1.00           1.32          5.77             263 -0.073846  \n",
       "13    0.61           1.76         14.29             213 -0.483913  \n",
       "14    1.00           1.81         19.58             182  0.326364  \n",
       "15    0.21           1.16          5.29              96  0.143042  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"houses.csv\") #reads the houses.csv file, which is in the same directory as this file\n",
    "l=len(df) #gets the amount of rows\n",
    "\n",
    "#Goes through each row to check for null values\n",
    "for i in range(0,l):\n",
    "    data=df.loc[i] #puts the specific row into an item called \"data\"\n",
    "    ll=len(data) #gets the length of a row\n",
    "    \n",
    "    #Checks each value in the row to see if it is null\n",
    "    for ii in range(0,ll):\n",
    "        if math.isnan(data[ii]):\n",
    "            df.drop(i,inplace=True) #drops the row with a null value\n",
    "            \n",
    "#Used the pandas documentation to try and reindex this and get the inplace above because it was not working otherwise because otherwise index 14 does not exist\n",
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\n",
    "df.reset_index(inplace=True,drop=True) #Resets the index for the dropped places\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Perform the appropriate statistical test at the $\\alpha = 0.01$ significance level to determine if _at least one_ of the features is related to the the response $y$.  Clearly describe your methodology and show all computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be following Notebook 22, using ols to get the $\\beta$s\n",
    "\n",
    "As for the test:\n",
    "\n",
    "$H_0$: $\\beta$=0 for all $\\beta$\n",
    "\n",
    "$H_1$: $\\beta$!=0 for at least one $\\beta$\n",
    "\n",
    "SST=$\\sum_{i=1}^{n} (y_i - \\bar{y})^2$\n",
    "\n",
    "SSE=$\\sum_{i=1}^{n} (y_i - \\hat{y})^2$ = $\\sum_{i=1}^{n} (y_i - (\\hat{\\beta_0}+\\hat{\\beta_1} x_1...+\\hat{\\beta_p} x_p))^2$\n",
    "\n",
    "F=$\\frac{\\frac{SST-SSE}{p}}{\\frac{SSE}{n-p-1}}$\n",
    "\n",
    "Methodology:\n",
    "\n",
    "Get the F statistic (using equations above)\n",
    "\n",
    "Convert the F statistic to the P-value\n",
    "\n",
    "Compare the p-value to the significance value for the hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is at least one significant value\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------Model-Variable-Setup----------------------------\n",
    "important=df[[\"age\",\"area\",\"bathrooms\",\"distance metro\",\"distance cemetery\",\"cats\",\"howls\",\"clouds\",\"precipitation\",\"misery index\",\"ice cream sold\"]]\n",
    "x=sm.add_constant(important)\n",
    "y=df[[\"haunted\"]]\n",
    "\n",
    "model=sm.OLS(y,x).fit() #Drawing of the model, giving us the betas as params\n",
    "\n",
    "ybar=df[\"haunted\"].mean() #Y-bar to calculate the SST\n",
    "\n",
    "SST=0 #initializer of sst to be summed up \n",
    "SSE=0 #initializer of sse to be summed up\n",
    "\n",
    "n=len(df) #number of samples, as the length of the dataframe\n",
    "p=len(model.params) #number of groups, calculated from the params (the 'haunted' is taken out and a beta_0 is added, so the groups are the same number)\n",
    "\n",
    "#For loop to go through the data and sum up the sst and sse\n",
    "for i in range(0,n):\n",
    "    yhat=0 #holder for the sum of all given values per sample\n",
    "    data=df.loc[i] #data holder for a specific trial\n",
    "    SST+=(data[p-1]-ybar)**2 #pulls the haunted value to make the sst for each loop\n",
    "    \n",
    "    #for loop to go through each part of the sample and bring in the betas to get the sse\n",
    "    for ii in range(0,p):\n",
    "        if ii!=0: #checks for all values that are not the constant beta\n",
    "            yhat+=model.params[ii]*data[ii-1] #mulitplies the non-constant beta by the xi for the sse\n",
    "        else:\n",
    "            yhat+=model.params[ii] #add in the constant\n",
    "    \n",
    "    SSE+=(data[ll-1]-yhat)**2 #actual computation of the SSE for each loop\n",
    "    \n",
    "num=(SST-SSE)/p #numerator for the f-statistic equation\n",
    "denom=SSE/(n-p-1) #denominator for the f-statistic\n",
    "f=num/denom #final f statistic\n",
    "\n",
    "pv=1-stats.f.cdf(f,p,n-p-1) #turning the f-statistic into a p-value\n",
    "\n",
    "if pv<0.01: #Checks if the p-value is less than the significant value. if so, there is at least one non-zero beta\n",
    "    print(\"There is at least one significant value\")\n",
    "else:\n",
    "    print(\"There is no significant beta value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Part C**: Write a function `forward_select(df, resp_str, maxk)` that takes in the DataFrame, the name of the column corresponding to the response, and the maximum number of desired features, and returns a list of feature names corresponding to the `maxk` most important features via forward selection.  At each stage in forward selection you should add the feature whose inclusion in the model would result in the lowest sum of squared errors $(SSE)$. Use your function to determine the best $k=5$ features to include in the model. Clearly indicate which feature was added in each stage. \n",
    "\n",
    "**Note**: The point of this exercise is to see if you can implement **foward_select** yourself.  You may of course use canned routines like statmodels OLS, but you may not call any Python method that explicitly performs forward selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE=$\\sum_{i=1}^{n} (y_i - \\hat{y})^2$ = $\\sum_{i=1}^{n} (y_i - (\\hat{\\beta_0}+\\hat{\\beta_1} x_1...+\\hat{\\beta_p} x_p))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to make the model, making it contained and easier to change if we want a different y parameter, etc later\n",
    "def make_model(df,res):\n",
    "    columns=np.array([df.columns.values]) #column names obtained from the dataframe\n",
    "    important=columns[columns!=res] #cancel out the one that is the response (y value)\n",
    "    imWithDf=df[important] #drawing values with those names (named after important just with the df because i forgot about it)\n",
    "    \n",
    "    #Model definitions\n",
    "    x=sm.add_constant(imWithDf)\n",
    "    y=df[[res]]\n",
    "\n",
    "    model=sm.OLS(y,x).fit() #Drawing of the model, giving us the betas as params\n",
    "    return model,important\n",
    "\n",
    "#Handles the Single Linear regression step of the forward select\n",
    "def slr(df,res,notY,model):\n",
    "    n=len(df) #the number of samples\n",
    "    p=len(notY) #The number of groups\n",
    "    haunted=df[res] #a list of the haunted values to use as yi\n",
    "    const=model.params[0] #The constant value of the relation\n",
    "    \n",
    "    feature=\"\" #Holder for the most significant feature\n",
    "    sse=1000 #Holder for the sse obtained, set to an absurd value to guarentee the first will snag it\n",
    "    \n",
    "    #For loop to compute sse's for every feature and display the most significant at the end\n",
    "    for i in range(0,p):\n",
    "        holder=0 #a holder for the sse of this feature\n",
    "        data=df[notY[i]] #a holder for the specific column\n",
    "        \n",
    "        #For loop to sum the sse\n",
    "        for ii in range(0,n):\n",
    "            newData=data.loc[ii] #Specific value xi\n",
    "            thisHaunted=haunted.loc[ii] #Specific value yi\n",
    "            holder+=(thisHaunted-(const+model.params[i+1]*newData))**2 #Computation of the sse given xi and yi\n",
    "        \n",
    "        if holder<sse: #Checks if this specific sse is the lowest\n",
    "            sse=holder #Sets the new lowest sse\n",
    "            feature=notY[i] #Holds the feature that is the lowest sse\n",
    "    return feature\n",
    "\n",
    "#A function to handle the p-1 multilinear calculations needed for the forward select\n",
    "def mlr(df,res,notY,model,arr):\n",
    "    n=len(df) #number for rows\n",
    "    p=len(notY) #number of columns\n",
    "    haunted=df[res] #holder for the values of the y value\n",
    "    const=model.params[0] #The constant value of the relation\n",
    "    \n",
    "    feature=\"\" #holder for the eventual decision of the feature\n",
    "    sse=1000 #holder for the best sse\n",
    "    e=0 #holder for the index value of the resulting item\n",
    "    b=0 #holder for the beta of the resulting item\n",
    "    \n",
    "    larr=len(arr) #length of the number we already went through\n",
    "    beta=np.zeros(larr) #holder of the betas of the ones we already counted\n",
    "    index=np.zeros(larr,dtype=int) #index values for the ones we have already taken into account\n",
    "    \n",
    "    count=0 #a counter that determines where in the arrays the values need to go\n",
    "    \n",
    "    #A while loop to continuously loop until we have filled all already-determined values\n",
    "    while count!=larr:\n",
    "        #a for loop to look for the final locations of already counted values\n",
    "        for i in range(0,p):\n",
    "            if notY[i]==arr[count]: #a check to see if this is the previously used column we are searching for\n",
    "                beta[count]=model.params[i+1] #Holder of the beta value for this specific instance\n",
    "                index[count]=i #Holder of the index of this specific instance\n",
    "                count+=1 #up the count so we can put the next set of values into the next slot of the arrays\n",
    "                break #end the current iteration, as we found where this value is\n",
    "            \n",
    "    #For loop to compare sse's to get the most optimal one\n",
    "    for i in range(0,p):\n",
    "        inde=index[index==i] #checks if this index is in the list of used indecies\n",
    "        \n",
    "        if len(inde)>0: #checks if this is indeed an index already used\n",
    "            continue #skip to the next column if we used this before\n",
    "            \n",
    "        holder=0 #a value to hold the current sse for later comparison\n",
    "        data=df[notY[i]] #holder of the current column\n",
    "        \n",
    "        #A for loop to calculate the sse of the current column by going through its xi's\n",
    "        for ii in range(0,n):\n",
    "            c=0 #a value to hold the summation just of the values we calculated previously\n",
    "            newData=data.loc[ii] #current column's xi value\n",
    "            thisHaunted=haunted.loc[ii] #the current yi for the given y\n",
    "            \n",
    "            #A for loop to simply add the portion of the relationship whose values we aready know best fit this\n",
    "            for ccc in range(0,larr):\n",
    "                c+=beta[ccc]*df[arr[ccc]].loc[ii] #calculation of beta_k x_k for values we have decided already fit best\n",
    "                \n",
    "            holder+=(thisHaunted-(const+c+model.params[i+1]*newData))**2 #sse for this column\n",
    "            \n",
    "        if holder<sse: #checks if this is the lowest sse we have calculated\n",
    "            sse=holder #the lower sse now becomes the new sse value\n",
    "            e=i #the index holder is now holding the index of this feature\n",
    "            b=model.params[i+1] #the beta value, given by the param at i+1 to offset for the constant\n",
    "            feature=notY[i] #the current best feature is recorded\n",
    "    \n",
    "    indexWin=np.array([e,b]) #puts the best index and the beta as an array so we can append it to the index and beta lists\n",
    "    index=np.append(index,indexWin[0]) #appending the index of the new value to the previous list\n",
    "    beta=np.append(beta,indexWin[1]) #appending the beta of the new value to the previous list\n",
    "    return feature,beta,index\n",
    "\n",
    "def forward_select(df,resp_str,maxk):\n",
    "    model,notY=make_model(df,resp_str) #creates the model\n",
    "    feature=slr(df,resp_str,notY,model) #gets the first feature from a simple linear relationship\n",
    "    theFeatures=np.array([feature]) #starts an array of the features\n",
    "    print(\"The first feature to be kept is {}\".format(feature)) #indicates the first added feature\n",
    "    \n",
    "    #for loop to get the rest of the features\n",
    "    for i in range(1,maxk):\n",
    "        feature,beta,index=mlr(df,resp_str,notY,model,theFeatures) #gets the next required feature\n",
    "        print(\"The next feature to be kept is {}\".format(feature)) #prints that feature, as in the instructions \"Clearly indicate which feature was added in each stage.\"\n",
    "        nextFeature=np.array([feature]) #put the new feature in an array so it can be appended\n",
    "        theFeatures=np.append(theFeatures,nextFeature) #appends the feature to the feature list\n",
    "        \n",
    "    #Print values to be used in part D  \n",
    "    print(\" \")\n",
    "    print(\"The betas are {} with a constant {}\".format(beta,model.params[0]))\n",
    "    print(\"The beta subscript (location) of these values is {}\".format(index))\n",
    "    return theFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first feature to be kept is distance cemetery\n",
      "The next feature to be kept is age\n",
      "The next feature to be kept is precipitation\n",
      "The next feature to be kept is bathrooms\n",
      "The next feature to be kept is howls\n",
      " \n",
      "The betas are [-0.09995715  0.00173249  0.00684016 -0.04109466  0.00564369] with a constant 0.3479565106233795\n",
      "The beta subscript (location) of these values is [4. 0. 8. 2. 6.]\n",
      "The features list returned from the forward_select method is ['distance cemetery' 'age' 'precipitation' 'bathrooms' 'howls']\n"
     ]
    }
   ],
   "source": [
    "features=forward_select(df,\"haunted\",5)\n",
    "print(\"The features list returned from the forward_select method is {}\".format(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write down the multiple linear regression model, including estimated parameters, obtained by your forward selection process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$y=\\beta_0 + \\beta_5 x_5 + \\beta_1 x_1 + \\beta_9 x_9 + \\beta_3 x_3 + \\beta_7 x_7$  < 1 added to each index to offset for $\\beta_0$ >\n",
    "\n",
    "$y=0.3480 - 0.1000x_5 + 0.0017 x_1 + 0.0068 x_9 - 0.0411 x_3 + 0.0056 x_7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Perform the appropriate statistical test at the $\\alpha = 0.05$ significance level to determine whether there is a statistically significant difference between the full model with all features and the reduced model obtained by forward selection in **Part D**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0$: All unused $\\beta$=0\n",
    "\n",
    "$H_1$: At least one unused $\\beta$ != 0\n",
    "\n",
    "n=number of instances\n",
    "\n",
    "p=number of groups/columns\n",
    "\n",
    "k=number of used terms in the reduced models (5 in this case)\n",
    "\n",
    "F=$\\frac{\\frac{(SSE_{reduced}-SSE_{full})}{p-k}}{\\frac{SSE_{full}}{n-p-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a statistically significant difference between the two models\n",
      "The p-value 0.014950622360453636 is less than the significance value 0.05\n"
     ]
    }
   ],
   "source": [
    "#a modified version of the forward select from before so I do not have to have all those annoying print statements\n",
    "def forward_select_modified(df,resp_str,maxk):\n",
    "    model,notY=make_model(df,resp_str) #creates the model\n",
    "    feature=slr(df,resp_str,notY,model) #gets the first feature from a simple linear relationship\n",
    "    theFeatures=np.array([feature]) #starts an array of the features\n",
    "    \n",
    "    #for loop to get the rest of the features\n",
    "    for i in range(1,maxk):\n",
    "        feature,beta,index=mlr(df,resp_str,notY,model,theFeatures) #gets the next required feature\n",
    "        nextFeature=np.array([feature]) #put the new feature in an array so it can be appended\n",
    "        theFeatures=np.append(theFeatures,nextFeature) #appends the feature to the feature list\n",
    "        \n",
    "    return theFeatures\n",
    "\n",
    "#models and features pulled from part c; if you wish to run this one, please run part c first\n",
    "model,notY=make_model(df,\"haunted\")\n",
    "features=forward_select_modified(df,\"haunted\",5)\n",
    "\n",
    "n=len(df) #The amount of samples tested\n",
    "p=len(notY) #The amount of columns that are not haunted (or another value)\n",
    "k=5 #The max k from before, which is the k in this case\n",
    "\n",
    "#A function to calculate then return the SSE of the reduced and full multilinear relationships\n",
    "def get_sse(df,res,model,arr):\n",
    "    larr=len(arr) #The amount of features\n",
    "    count=0 #a count variable to get the betas and indexes for the chosen features\n",
    "    \n",
    "    beta=np.zeros(larr) #holder of the betas of the ones we already counted\n",
    "    index=np.zeros(larr,dtype=int) #index values for the ones we have already taken into account\n",
    "    \n",
    "    #A while loop to continuously loop for the index and betas for the values. Brought in from part c for a similar purpose\n",
    "    while count!=larr:\n",
    "        #a for loop to look for the final locations of already counted values\n",
    "        for i in range(0,p):\n",
    "            if notY[i]==arr[count]: #a check to see if this is the previously used column we are searching for\n",
    "                beta[count]=model.params[i+1] #Holder of the beta value for this specific instance\n",
    "                index[count]=i #Holder of the index of this specific instance\n",
    "                count+=1 #up the count so we can put the next set of values into the next slot of the arrays\n",
    "                break #end the current iteration, as we found where this value is\n",
    "                \n",
    "    sseFull=0 #holder for the full SSE\n",
    "    sseRed=0 #holder for the SSE of the reduced model\n",
    "    const=model.params[0] #The constant value\n",
    "    \n",
    "    #A for loop to go through the sample's values and calculate their SSE's\n",
    "    for i in range(0,n):\n",
    "        data=df.loc[i] #Holder for the data for one specific instance\n",
    "        haunted=df[res].loc[i] #This loop's value for haunted (or another instance)\n",
    "        holder=0 #Holder of this loop's SSE for the full SSE\n",
    "        holderRed=0 #Holder of this loop's SSE for the reduced SSE\n",
    "        \n",
    "        #For loop to compute the SSE for this loop's full SSE\n",
    "        for ii in range(0,p):\n",
    "            holder+=data[ii]*model.params[ii+1] #beta_1 x_1 + beta_2 x_2...\n",
    "            \n",
    "        sseFull+=(haunted-(holder+const))**2 #calculation of the full SSE\n",
    "        \n",
    "        #For loop to compute the SSE for the specific featured values\n",
    "        for ii in range(0,larr):\n",
    "            e=notY[index[ii]] #Taking the specific column we need to draw\n",
    "            c=df[e].loc[i] #Getting the value of that column at the ith row\n",
    "            holderRed+=beta[ii]*c #beta_k x_k\n",
    "            \n",
    "        sseRed+=(haunted-(holderRed+const))**2 #Calculation of the reduced model's SSE\n",
    "        \n",
    "    return sseFull,sseRed\n",
    "        \n",
    "sseFull,sseRed=get_sse(df,\"haunted\",model,features) #Call to calculate the SSE's\n",
    "\n",
    "num=(sseRed-sseFull)/(p-k) #Numerator to the f statistic\n",
    "denom=sseFull/(n-p-1) #Denominator to the f statistic\n",
    "f=num/denom #getting the f\n",
    "p=1-stats.f.cdf(f,p-k,n-p-1) #Obtaining the p value to test if all is fine and dandy\n",
    "\n",
    "if p<=0.05: #Test if there is a significant difference between the two models\n",
    "    print(\"There is a statistically significant difference between the two models\")\n",
    "    print(\"The p-value {} is less than the significance value 0.05\".format(p))\n",
    "else:\n",
    "    print(\"There is NO statistically significant difference between the two models\")\n",
    "    print(\"The p-value {} is greater than the significance value 0.05\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Based on your conclusions in **Part E**, use the _better_ of the two models to predict the haunted house hazard when the following features are observed: \n",
    "\n",
    "- $\\texttt{age}$: 100 years\n",
    "- $\\texttt{area}$: 2200 square feet\n",
    "- $\\texttt{bathrooms}$: 3 bathrooms\n",
    "- $\\texttt{distance metro}$: 25 miles\n",
    "- $\\texttt{distance cemetery}$: 10 miles\n",
    "- $\\texttt{cats}$: 4 cats\n",
    "- $\\texttt{howls}$: 5 wolf howls/night\n",
    "- $\\texttt{clouds}$: 0.65 cloud cover\n",
    "- $\\texttt{precipitation}$: 0 inches\n",
    "- $\\texttt{misery index}$: 10\n",
    "- $\\texttt{ice cream sold}$: 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The haunted value is -0.7782125496062187\n"
     ]
    }
   ],
   "source": [
    "#Creation of the model using the function defined in part C\n",
    "model,notY=make_model(df,\"haunted\")\n",
    "\n",
    "newValues=np.array([100,2200,3,25,10,4,5,0.65,0,10,125]) #Array of the values given above\n",
    "\n",
    "l=len(newValues) #The amount of values to test\n",
    "summ=0 #summator to get the haunted value of y\n",
    "\n",
    "#For loop to sum the xi multiplied by their betas\n",
    "for i in range(0,l):\n",
    "    summ+=newValues[i]*model.params[i+1] #beta_k x_k\n",
    "    \n",
    "haunted=summ+model.params[0] #Adding the constant value to get the haunted value\n",
    "\n",
    "print(\"The haunted value is {}\".format(haunted)) #print the haunted value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G:** Governor Polis dabbles a bit in the art of data science, as well as the science of data art. He tells you that the response (`haunted` score) that you and Amy predicted is actually the natural logarithm of the _odds_ that a house with the given features is haunted, where if $p$ is the probability that a house is haunted, then the odds are given by $$\\text{odds} = \\dfrac{p}{1-p}$$\n",
    "\n",
    "What transformation must you make in order to turn your multiple regression model into a logistic regression model, to classify a house a haunted or not? Perform this simple transformation, then use a decision threshold of 0.5 to classify the house from **Part F** as haunted or not haunted. No new models should be fit here; use the same model that you used in Part F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p is the probability we need\n",
    "\n",
    "odds=$\\frac{p}{1-p}$\n",
    "\n",
    "odds(1-p)=p\n",
    "\n",
    "odds-podds=p\n",
    "\n",
    "$\\frac{odds}{p}$ - odds=1\n",
    "\n",
    "$\\frac{odds}{p}$=1+odds\n",
    "\n",
    "$\\frac{p}{odds}$=$\\frac{1}{1+odds}$\n",
    "\n",
    "p=$\\frac{odds}{1+odds}$\n",
    "\n",
    "odds=$e^{\\beta_0+\\beta_1 x_1+...+\\beta_p x_p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability is 0.3147052503432849\n",
      "As a sanity check, the probability is 0.3147052503432849 using the sigmoid equation instead\n",
      "This probability is lower than the threshold of 0.5, so it is likely not haunted\n"
     ]
    }
   ],
   "source": [
    "model,notY=make_model(df,\"haunted\") #Creation of the model so we can use the betas\n",
    "p=len(newValues) #number of fields to test\n",
    "const=model.params[0] #the constant value so we can use it later\n",
    "summ=0 #a summation variable to get the multilinear equation\n",
    "\n",
    "newValues=np.array([100,2200,3,25,10,4,5,0.65,0,10,125]) #Array of the values given in the previous part\n",
    "\n",
    "#For loop to get the values of the equation with the given betas\n",
    "for ii in range(0,p):\n",
    "    summ+=newValues[ii]*model.params[ii+1] #beta_k x_k\n",
    "    \n",
    "odds=math.exp(const+summ) #calculation of odds using the odds equation\n",
    "pw=odds/(1+odds) #Getting the probability with the transformation calculated above\n",
    "ps=1/(1+math.exp(-(const+summ))) #getting the probability with the sigmoid function as a sanity check\n",
    "\n",
    "print(\"The probability is {}\".format(pw)) #Print the probability gotten from the odds conversion\n",
    "print(\"As a sanity check, the probability is {} using the sigmoid equation instead\".format(ps)) #sanity check with the sigmoid function\n",
    "\n",
    "if pw>=0.5: #checks if the value is above or below our 0.5 threshold\n",
    "    print(\"This probability is greater than the threshold of 0.5, so it is likely haunted\")\n",
    "else:\n",
    "    print(\"This probability is lower than the threshold of 0.5, so it is likely not haunted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "<a id='p2'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "### [30 points] Problem 2: Definitions check \n",
    "\n",
    "\n",
    "Let $Z \\sim N(0, 1)$ and define $W = Z^2$. It is clear that $W$ and $Z$ are not\n",
    "independent (clearly $W$ depends on $Z$).\n",
    "\n",
    "Show that $Cov(W, Z) = 0$ even though $W$ and $Z$ are\n",
    "dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cov(W,Z)=E[(W-E[W]) * (Z-E[Z])]\n",
    "\n",
    "Cov(W,Z)=E[($Z^2$-E[$Z^2$]) * (Z-E[Z])]\n",
    "\n",
    "E[x]=$\\int_{-\\infty}^{\\infty} x f(x) dx$\n",
    "\n",
    "Cov(W,Z)=E[($Z^2$-$\\int_{-\\infty}^{\\infty} z(z^2) dz$) * (Z-$\\int_{-\\infty}^{\\infty} z(z) dz$)]\n",
    "\n",
    "---\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} z^3 dz$\n",
    "\n",
    "$\\frac{1}{4} z^4 | -\\infty to \\infty$\n",
    "\n",
    "$\\infty - \\infty$\n",
    "\n",
    "(Realizes $\\infty - \\infty$ is not 0)\n",
    "\n",
    "---\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} z^2 dz$\n",
    "\n",
    "$\\frac{1}{3} z^3 | -\\infty to \\infty$\n",
    "\n",
    "$\\infty - (-\\infty)$\n",
    "\n",
    "$\\infty + \\infty$\n",
    "\n",
    "$\\infty$\n",
    "\n",
    "---\n",
    "\n",
    "Cov(W,Z)=E[($Z^2-(\\infty-\\infty)$) * (Z-$\\infty$)]\n",
    "\n",
    "Cov(W,Z)=E[($Z^2-(\\infty-\\infty)$) * (-$\\infty$)]\n",
    "\n",
    "($\\infty-\\infty$ is just some constant, thus can still be absorbed with $Z^2$ into the -$\\infty$)\n",
    "\n",
    "Cov(W,Z)=E[(-$\\infty)$]\n",
    "\n",
    "Cov(W,Z)=$\\int_{-\\infty}^{\\infty} -\\infty$ dz\n",
    "\n",
    "Cov(W,Z)=0 | $-\\infty to \\infty$\n",
    "\n",
    "Cov(W,Z)=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='bot'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
