{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6988343e",
   "metadata": {},
   "source": [
    "# CSCI 5622: Machine Learning\n",
    "## Fall 2023\n",
    "### Instructor: Daniel Acuna, Associate Professor, Department of Computer Science, University of Colorado at Boulder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fdf184",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e51cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Luna McBride\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1b70e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a996eb-9dfe-40be-bbd8-6eae4cb4e0db",
   "metadata": {},
   "source": [
    "# Homework 6 - Topic Modeling (50 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1872a5f-23fc-42c6-865a-2b7f1111b7c7",
   "metadata": {},
   "source": [
    "## Question 1: (10 pts) Dataset Acquisition and Preprocessing\n",
    "\n",
    "**Objective:** In this question, you will acquire a dataset of text and perform preprocessing steps to prepare it for topic modeling using scikit-learn. This preprocessing step is crucial for effective topic modeling using algorithms like NMF (Non-negative Matrix Factorization) and LDA (Latent Dirichlet Allocation).\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. **Dataset Acquisition:** \n",
    "   - Download the '20 Newsgroups' dataset, a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups.\n",
    "   - URL for the dataset: `http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz`\n",
    "   - Use the `fetch_20newsgroups` function from `sklearn.datasets` to load the dataset. \n",
    "   - Focus on a subset of 4 newsgroups for simplicity: `['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']`.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - Tokenize and extract features from the text data using `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
    "   - Perform the following preprocessing steps:\n",
    "     - Convert all text to lowercase.\n",
    "     - Remove stopwords.\n",
    "     - Use a `max_df` of 0.95 and `min_df` of 2.\n",
    "     - Extract the top 1000 most frequent words.\n",
    "\n",
    "Use the test cell to guide you as to which variables to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ef8df0-4a2c-4318-a96d-5671d13fcc45",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df4f447d7f9e40a18da7495fe6901472",
     "grade": false,
     "grade_id": "cell-d2f81db86b4b04f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "newsgroups_data = fetch_20newsgroups() #Get the training data from the newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d91ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      " From: jgreen@amber (Joe Green)\n",
      "Subject: Re: Weitek P9000 ?\n",
      "Organization: Harris Computer Systems Division\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: amber.ssd.csd.harris.com\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "--\n",
      "Joe Green\t\t\t\tHarris Corporation\n",
      "jgreen@csd.harris.com\t\t\tComputer Systems Division\n",
      "\"The only thing that really scares me is a person with no sense of humor.\"\n",
      "\t\t\t\t\t\t-- Jonathan Winters\n",
      "\n",
      "\n",
      " Type: \n",
      " comp.graphics\n"
     ]
    }
   ],
   "source": [
    "subset = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'] #Get the list of topics from above\n",
    "\n",
    "newsgroups_actual_data = [] #Holder for the actual data\n",
    "newsgroups_types = [] #Holder for the target types\n",
    "\n",
    "#For each piece of data, add it if it is in the expected subset of topics above\n",
    "for i in range(len(newsgroups_data.data)):\n",
    "    data = newsgroups_data.data[i] #Get the data\n",
    "    target = newsgroups_data.target[i] #Get the actual target topic\n",
    "    types = newsgroups_data.target_names[target] #Get the name of the topic using the target value\n",
    "    \n",
    "    #If the type is in the subset of types, add it to the data holder lists\n",
    "    if types in subset:\n",
    "        newsgroups_actual_data.append(data) #Add the data to the data holder list\n",
    "        newsgroups_types.append(types) #Add the topic to the types list\n",
    "        \n",
    "#Print out an example to show off the data structure\n",
    "print(\"Data: \\n\", newsgroups_actual_data[0])\n",
    "print(\"\\n Type: \\n\", newsgroups_types[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165e27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.95 #Set the max_df\n",
    "min_df = 2 #Set the min_df\n",
    "top_words = 1000 #Set the max number of words\n",
    "\n",
    "#Initialize the Tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(lowercase = True, stop_words = \"english\", max_df = max_df, min_df = min_df, max_features = top_words)\n",
    "preprocessed_data = tfidf.fit_transform(newsgroups_actual_data) #Get the preprocessed data to throw into the next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e53227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '01' '04' '10' '100' '11' '12' '128' '13' '14' '15' '16' '17'\n",
      " '18' '19' '1993' '1993apr15' '20' '200' '2000' '21' '22' '23' '24' '25'\n",
      " '256' '26' '27' '28' '29' '30' '31' '32' '33' '35' '3d' '40' '41' '42'\n",
      " '50' '93' '__' '___' '_____' 'able' 'ac' 'accept' 'access' 'according'\n",
      " 'act' 'action' 'actions' 'activities' 'acts' 'actually' 'ad' 'add'\n",
      " 'added' 'address' 'advance' 'age' 'agency' 'ago' 'agree' 'air' 'alaska'\n",
      " 'algorithm' 'allan' 'allen' 'allow' 'allowed' 'alt' 'american' 'ames'\n",
      " 'amiga' 'analysis' 'ancient' 'andrew' 'animals' 'animation' 'anonymous'\n",
      " 'answer' 'answers' 'anti' 'anybody' 'apparently' 'apple' 'applications'\n",
      " 'apply' 'appreciated' 'apr' 'april' 'archive' 'area' 'aren' 'argument'\n",
      " 'arguments' 'arizona' 'article' 'articles' 'ask' 'asked' 'assume' 'astro'\n",
      " 'astronomy' 'atheism' 'atheist' 'atheists' 'atmosphere' 'attempt' 'au'\n",
      " 'aurora' 'australia' 'author' 'authority' 'available' 'away' 'baalke'\n",
      " 'bad' 'base' 'based' 'basic' 'basically' 'basis' 'bbs' 'beauchaine'\n",
      " 'behavior' 'belief' 'beliefs' 'believe' 'benedikt' 'berkeley' 'best'\n",
      " 'better' 'bible' 'biblical' 'big' 'billion' 'bit' 'bitnet' 'bits' 'black'\n",
      " 'blood' 'board' 'bob' 'bobbe' 'bobby' 'body' 'book' 'books' 'box' 'brian'\n",
      " 'bu' 'buffalo' 'build' 'building' 'built' 'business' 'buy' 'ca'\n",
      " 'california' 'called' 'caltech' 'came' 'canada' 'card' 'care' 'case'\n",
      " 'cause' 'caused' 'cc' 'cco' 'cd' 'center' 'century' 'certain' 'certainly'\n",
      " 'ch' 'change' 'check' 'child' 'children' 'choice' 'chris' 'christ'\n",
      " 'christian' 'christianity' 'christians' 'church' 'city' 'claim' 'claims'\n",
      " 'clear' 'clearly' 'cleveland' 'close' 'cmu' 'code' 'college' 'color'\n",
      " 'colors' 'com' 'come' 'comes' 'comet' 'command' 'comments' 'commercial'\n",
      " 'common' 'communications' 'community' 'comp' 'company' 'complete'\n",
      " 'complex' 'computer' 'computing' 'concept' 'conclusion' 'consider'\n",
      " 'considered' 'contact' 'contains' 'context' 'control' 'convert' 'copy'\n",
      " 'corp' 'corporation' 'correct' 'cost' 'costs' 'couldn' 'country' 'couple'\n",
      " 'course' 'create' 'created' 'cs' 'cso' 'current' 'currently' 'cview'\n",
      " 'cwru' 'data' 'date' 'dave' 'david' 'day' 'days' 'dc' 'dead' 'deal'\n",
      " 'death' 'define' 'defined' 'definition' 'deleted' 'den' 'department'\n",
      " 'dept' 'described' 'description' 'design' 'designed' 'details'\n",
      " 'developed' 'development' 'did' 'didn' 'die' 'died' 'different' 'digex'\n",
      " 'direct' 'directory' 'disclaimer' 'discussion' 'display' 'distribution'\n",
      " 'does' 'doesn' 'doing' 'don' 'dos' 'doubt' 'dr' 'draw' 'drive' 'driver'\n",
      " 'dseg' 'dwyer' 'earlier' 'early' 'earth' 'easily' 'east' 'easy' 'ed'\n",
      " 'edu' 'email' 'end' 'energy' 'eng' 'engin' 'engineering' 'environment'\n",
      " 'eric' 'error' 'especially' 'eternal' 'event' 'events' 'evidence' 'evil'\n",
      " 'exactly' 'example' 'exist' 'existence' 'exists' 'expect' 'experience'\n",
      " 'explain' 'exploration' 'face' 'fact' 'faith' 'fall' 'fallacy' 'false'\n",
      " 'faq' 'far' 'fast' 'father' 'fax' 'feel' 'fi' 'field' 'file' 'files'\n",
      " 'fine' 'fit' 'flight' 'fnal' 'follow' 'following' 'force' 'form' 'format'\n",
      " 'formats' 'frank' 'fred' 'free' 'freedom' 'freenet' 'ftp' 'function'\n",
      " 'funding' 'future' 'galileo' 'gas' 'general' 'generally' 'germany' 'gets'\n",
      " 'getting' 'gif' 'given' 'gmt' 'goal' 'god' 'gods' 'goes' 'going' 'good'\n",
      " 'got' 'gov' 'government' 'graphics' 'gravity' 'great' 'greek' 'gregg'\n",
      " 'ground' 'group' 'groups' 'guess' 'guy' 'halat' 'half' 'hand' 'happened'\n",
      " 'happy' 'hard' 'hardware' 'haven' 'having' 'head' 'hear' 'heard' 'held'\n",
      " 'hell' 'help' 'henry' 'hi' 'higgins' 'high' 'higher' 'history' 'hold'\n",
      " 'holy' 'home' 'homosexuality' 'hope' 'host' 'hours' 'house' 'hp' 'hudson'\n",
      " 'human' 'humans' 'ibm' 'ico' 'idea' 'ideas' 'ignorance' 'image' 'images'\n",
      " 'imagine' 'imaging' 'important' 'include' 'included' 'includes'\n",
      " 'including' 'individual' 'info' 'information' 'innocent' 'inside'\n",
      " 'instead' 'institute' 'interested' 'interesting' 'interface'\n",
      " 'international' 'internet' 'interpretation' 'involved' 'islam' 'islamic'\n",
      " 'isn' 'issue' 'jaeger' 'james' 'jesus' 'jet' 'jewish' 'jews' 'jim' 'job'\n",
      " 'john' 'jon' 'jpeg' 'jpl' 'jsc' 'judas' 'jupiter' 'just' 'keith' 'kelvin'\n",
      " 'ken' 'kent' 'key' 'keywords' 'kill' 'killed' 'killing' 'kind' 'king'\n",
      " 'km' 'kmr4' 'know' 'knowledge' 'known' 'knows' 'koresh' 'lab'\n",
      " 'laboratory' 'lack' 'land' 'language' 'large' 'later' 'launch' 'launched'\n",
      " 'law' 'laws' 'learn' 'leave' 'left' 'let' 'level' 'levels' 'liar'\n",
      " 'library' 'life' 'light' 'like' 'likely' 'line' 'list' 'little' 'live'\n",
      " 'livesey' 'living' 'll' 'local' 'logic' 'long' 'longer' 'look' 'looking'\n",
      " 'looks' 'lord' 'loss' 'lot' 'love' 'low' 'lunar' 'mac' 'machine' 'mail'\n",
      " 'major' 'majority' 'make' 'makes' 'making' 'man' 'mantis' 'mark' 'market'\n",
      " 'mars' 'mary' 'mass' 'material' 'math' 'mathew' 'matter' 'matthew'\n",
      " 'maybe' 'mccall' 'mean' 'meaning' 'means' 'media' 'members' 'memory'\n",
      " 'men' 'mention' 'mentioned' 'merely' 'message' 'michael' 'mil' 'military'\n",
      " 'million' 'mind' 'mission' 'missions' 'mit' 'mode' 'model' 'models'\n",
      " 'modern' 'monash' 'money' 'moon' 'moral' 'morality' 'motto' 'mr' 'ms'\n",
      " 'murder' 'muslim' 'muslims' 'nasa' 'national' 'natural' 'nature' 'navy'\n",
      " 'near' 'necessarily' 'necessary' 'need' 'needed' 'needs' 'net' 'netcom'\n",
      " 'network' 'new' 'news' 'newsgroup' 'newsreader' 'newton' 'nice' 'nick'\n",
      " 'night' 'nl' 'nntp' 'non' 'north' 'note' 'nsmca' 'number' 'numbers'\n",
      " 'object' 'objective' 'objects' 'observations' 'observatory' 'obvious'\n",
      " 'obviously' 'office' 'oh' 'ok' 'okcforum' 'old' 'ones' 'online' 'open'\n",
      " 'opinion' 'opinions' 'orbit' 'orbital' 'order' 'org' 'original' 'outside'\n",
      " 'package' 'paper' 'particular' 'pasadena' 'past' 'pat' 'paul' 'pay' 'pc'\n",
      " 'peace' 'people' 'perfect' 'period' 'person' 'personal' 'peter' 'phone'\n",
      " 'physical' 'physics' 'picture' 'place' 'places' 'plan' 'plane' 'planet'\n",
      " 'planetary' 'po' 'point' 'points' 'political' 'polygon' 'position'\n",
      " 'possible' 'possibly' 'post' 'posted' 'posting' 'postings' 'posts'\n",
      " 'postscript' 'power' 'prb' 'present' 'press' 'pretty' 'princeton'\n",
      " 'private' 'probably' 'probe' 'problem' 'problems' 'process' 'processing'\n",
      " 'program' 'programming' 'programs' 'project' 'propulsion' 'prove'\n",
      " 'provide' 'provides' 'pub' 'public' 'punishment' 'purpose' 'quality'\n",
      " 'question' 'questions' 'quicktime' 'quite' 'quote' 'qur' 'ra' 'radio'\n",
      " 'radius' 'rate' 'ray' 'read' 'reading' 'real' 'reality' 'really' 'reason'\n",
      " 'reasonable' 'reasons' 'recently' 'reference' 'references' 'related'\n",
      " 'religion' 'religions' 'religious' 'remember' 'remote' 'reply' 'report'\n",
      " 'request' 'require' 'required' 'requires' 'research' 'resources'\n",
      " 'respect' 'response' 'responsible' 'rest' 'result' 'results' 'return'\n",
      " 'rice' 'right' 'rights' 'robert' 'rocket' 'rockets' 'ron' 'rule' 'rules'\n",
      " 'run' 'running' 'runs' 'rushdie' 'russian' 'ryan' 'said' 'san' 'sandvik'\n",
      " 'satan' 'satellite' 'satellites' 'saturn' 'save' 'saw' 'say' 'saying'\n",
      " 'says' 'scale' 'schneider' 'school' 'sci' 'science' 'scientific' 'screen'\n",
      " 'se' 'second' 'section' 'seen' 'self' 'send' 'sense' 'sent' 'series'\n",
      " 'seriously' 'server' 'service' 'services' 'set' 'sex' 'sgi' 'shall'\n",
      " 'short' 'shuttle' 'similar' 'simple' 'simply' 'sin' 'single' 'site'\n",
      " 'sites' 'situation' 'size' 'sky' 'small' 'society' 'software' 'solar'\n",
      " 'solntze' 'sorry' 'sort' 'sound' 'sounds' 'source' 'sources' 'space'\n",
      " 'spacecraft' 'speak' 'special' 'species' 'specific' 'specifically'\n",
      " 'speed' 'spencer' 'spent' 'sphere' 'st' 'stage' 'standard' 'standards'\n",
      " 'stanford' 'star' 'start' 'started' 'state' 'stated' 'statement' 'states'\n",
      " 'station' 'stephen' 'steve' 'stop' 'stories' 'story' 'street' 'strong'\n",
      " 'structure' 'student' 'study' 'stuff' 'summary' 'sun' 'support' 'sure'\n",
      " 'surface' 'systems' 'taken' 'takes' 'taking' 'talk' 'talking' 'technical'\n",
      " 'technology' 'tek' 'telescope' 'tell' 'term' 'test' 'testament' 'texas'\n",
      " 'text' 'thank' 'thanks' 'theory' 'thing' 'things' 'think' 'thinking'\n",
      " 'thomas' 'thought' 'thoughts' 'thread' 'ti' 'tiff' 'time' 'times' 'tin'\n",
      " 'titan' 'today' 'told' 'tom' 'tony' 'took' 'tool' 'tools' 'toronto'\n",
      " 'total' 'tried' 'true' 'truth' 'try' 'trying' 'tu' 'turn' 'type' 'tyre'\n",
      " 'uiuc' 'uk' 'umd' 'umich' 'unc' 'understand' 'understanding'\n",
      " 'unfortunately' 'universe' 'university' 'unix' 'unless' 'usa' 'use'\n",
      " 'used' 'useful' 'usenet' 'user' 'users' 'uses' 'using' 'usually' 'utexas'\n",
      " 'uucp' 'valid' 'value' 'values' 'van' 'various' 'vax' 've' 'vehicle'\n",
      " 'venus' 'version' 'vesa' 'vga' 'vice' 'video' 'view' 'viewer' 'views'\n",
      " 'virginia' 'virtual' 'vms' 'volume' 'want' 'wanted' 'wants' 'war'\n",
      " 'washington' 'wasn' 'way' 'ways' 'week' 'went' 'west' 'western' 'white'\n",
      " 'willing' 'windows' 'wish' 'women' 'won' 'wonder' 'word' 'words' 'work'\n",
      " 'working' 'works' 'world' 'worse' 'worth' 'wouldn' 'wpd' 'write' 'writes'\n",
      " 'writing' 'written' 'wrong' 'wrote' 'year' 'years' 'yes' 'young' 'zoo'\n",
      " 'zoology']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.get_feature_names_out()) #Print out the common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778d6a92-07ea-4fed-88ea-b1f952bbafd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25f0ddee46fbd1e05ec7bb888435a768",
     "grade": true,
     "grade_id": "cell-f101fd787a16e591",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 pts\n",
    "def test_dataset_download():\n",
    "    global newsgroups_data\n",
    "    \n",
    "    assert 'newsgroups_data' in globals(), \"Dataset not loaded with the variable name 'newsgroups_data'\"\n",
    "    assert len(newsgroups_data.data) > 0, \"Dataset seems to be empty\"\n",
    "\n",
    "def test_preprocessing():\n",
    "    global tfidf, processed_features\n",
    "    \n",
    "    assert 'tfidf' in globals(), \"TfidfVectorizer not defined\"\n",
    "    assert hasattr(tfidf, 'fit_transform'), \"TfidfVectorizer not properly initialized\"\n",
    "    assert tfidf.get_feature_names_out().shape[0] == 1000, \"The number of features extracted does not match 1000\"\n",
    "\n",
    "# Run the tests\n",
    "test_dataset_download()\n",
    "test_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad5636-6d89-44f1-9411-118f5eb4ba8a",
   "metadata": {},
   "source": [
    "## Question 2: Non-negative Matrix Factorization (NMF) and Performance Validation\n",
    "\n",
    "**Objective:** Apply NMF to the preprocessed text dataset with different numbers of components (topics) and evaluate the performance using a specific metric. This exercise will help you understand the impact of choosing different dimensions (number of topics) in topic modeling.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. **Implement NMF:**\n",
    "   - Apply NMF on the preprocessed text data (from Question 1) using scikit-learn's `NMF` class.\n",
    "   - Experiment with different numbers of components (use 5, 10, 15, 20).\n",
    "   - Use the 'frobenius' norm as the loss function and a random state of 42 for reproducibility.\n",
    "\n",
    "2. **Performance Validation:**\n",
    "   - Evaluate the performance of each NMF model using the Frobenius norm of the matrix difference (i.e., the difference between the original data matrix and the reconstructed matrix from the NMF components and coefficients).\n",
    "   - Store the Frobenius norm values for each number of components in a list or dictionary for comparison.\n",
    "\n",
    "Look at the test to determine where you have to save the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdab92ea-1d3c-4cc3-89d7-97d8f756b7c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495755304501ea4d81e1f7b3b2a4ad64",
     "grade": false,
     "grade_id": "cell-922189026b8a5a2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "nmf_models = {} #Initialize the nmf model dictionaries\n",
    "frobenius_norms = {} #Initialize the frobenius norm dictionary\n",
    "\n",
    "#For each step of 5 up to 20, train the model with this number of components\n",
    "for i in range(5, 25, 5):\n",
    "    model = NMF(i, beta_loss = \"frobenius\", random_state = 42) #Initialize the model\n",
    "    nmf_models[i] = model #Add the model to the list\n",
    "    model.fit_transform(preprocessed_data) #Fit the model on the data\n",
    "    \n",
    "    frobenius_norms[i] = model.reconstruction_err_ #Get the frobenius norms from the model\n",
    "    \n",
    "\n",
    "#Dictionary Component Source: https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "optimal_components = max(frobenius_norms, key = frobenius_norms.get) #Get the optimal components by getting the max of the frobenius norms\n",
    "print(optimal_components) #Print the optimal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0672f77-a503-458e-805b-7191aa118a89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b08d1b1aa677a8fa8f8bfbb67088201",
     "grade": true,
     "grade_id": "cell-b3b0e2af1b7489c4",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 15 pts\n",
    "def test_nmf_models():\n",
    "    assert 'nmf_models' in globals(), \"nmf_models dictionary not defined\"\n",
    "    assert isinstance(nmf_models, dict), \"nmf_models should be a dictionary\"\n",
    "    assert all(isinstance(nmf, NMF) for nmf in nmf_models.values()), \"All values in nmf_models should be instances of NMF\"\n",
    "    assert all(n_components in nmf_models for n_components in [5, 10, 15, 20]), \"NMF models for all specified component numbers should be created\"\n",
    "\n",
    "def test_frobenius_norms():\n",
    "    assert 'frobenius_norms' in globals(), \"frobenius_norms dictionary not defined\"\n",
    "    assert isinstance(frobenius_norms, dict), \"frobenius_norms should be a dictionary\"\n",
    "    assert len(frobenius_norms) == 4, \"There should be four Frobenius norm values for the four NMF models\"\n",
    "    assert all(isinstance(norm, float) for norm in frobenius_norms.values()), \"All values in frobenius_norms should be floats\"\n",
    "\n",
    "def test_optimal_components_selection():\n",
    "    assert 'optimal_components' in globals(), \"Variable 'optimal_components' not defined\"\n",
    "    assert optimal_components in [5, 10, 15, 20], \"Optimal components not selected from the predefined list\"\n",
    "\n",
    "# Run the tests\n",
    "test_nmf_models()\n",
    "test_frobenius_norms()\n",
    "test_optimal_components_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6cb15-8c43-48f9-a7b4-fe6611193d77",
   "metadata": {},
   "source": [
    "## Question 3: Latent Dirichlet Allocation (LDA) and Topic Interpretation\n",
    "\n",
    "**Objective:** Apply LDA to the preprocessed text dataset from Question 1, extract 5 topics, and interpret what each topic represents. Use `CountVectorizer` instead of TF-IDF.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. **Implement LDA:**\n",
    "   - Apply LDA on the preprocessed text data using scikit-learn's `LatentDirichletAllocation` class.\n",
    "   - Extract exactly 5 topics from the dataset.\n",
    "\n",
    "2. **Print and Interpret Topics:**\n",
    "   - For each topic, print the top 10 words based on their importance in the topic.\n",
    "   - Write a brief interpretation for each topic, discussing what you think the topic represents based on the top words.\n",
    "\n",
    "Use the test cell you guide about the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702529af-2be0-4763-9074-b8d40bbcf1f3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d086eda0542476a32d4075999bc272a",
     "grade": false,
     "grade_id": "cell-593413c409e54083",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40.13508121  11.79621799  19.2516298  ...   7.86754791   1.22819004\n",
      "    0.20005377]\n",
      " [  0.2014505   36.8608462   15.67361571 ...  17.65254265   0.20083313\n",
      "    0.20057814]\n",
      " [ 35.19403892 127.5287325   30.6544995  ...   5.35969355 119.16600329\n",
      "   67.19904811]\n",
      " [  0.20209784   0.20134425   0.20205368 ...  12.69297176   0.20322138\n",
      "    0.20005438]\n",
      " [  8.26733152   8.61285907   0.21820131 ...  32.42724413   0.20175217\n",
      "    0.20026559]]\n"
     ]
    }
   ],
   "source": [
    "# 20 pts\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#Vectorize the count vector with the same stipulations as the other vectorizer\n",
    "count_vectorizer = CountVectorizer(lowercase = True, stop_words = \"english\", max_df = max_df, min_df = min_df, max_features = top_words)\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(newsgroups_actual_data) #Fit of the data\n",
    "terms = count_vectorizer.get_feature_names_out() #Get the keywords from the vectorizer\n",
    "\n",
    "lda_model = LatentDirichletAllocation(5, random_state = 42) #Initialize the LDA model\n",
    "fit_model = lda_model.fit(count_vectorizer_fit) #Fit the model on the fit vectorizer\n",
    "\n",
    "features = fit_model.get_feature_names_out() #Take out the features\n",
    "\n",
    "print(fit_model.components_) #Print out the components to show off the structure\n",
    "\n",
    "# Hypothetical interpretations:\n",
    "# Topic 1: Likely about space exploration (words like 'nasa', 'space', 'orbit')\n",
    "# Topic 2: Computer technology (words like 'software', 'graphics', 'image')\n",
    "# Topic 3: Religion and philosophy (words like 'god', 'morality', 'belief')\n",
    "# Topic 4: Online communities and discussion (words like 'internet', 'email', 'group')\n",
    "# Topic 5: Science and research (words like 'data', 'study', 'theory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a53912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edu', 'graphics', 'image', 'file', 'files', 'university', 'software', 'images', 'data', 'ftp']\n",
      "['people', 'jesus', 'like', 'don', 'just', 'know', 'say', 'good', 'think', 'life']\n",
      "['space', 'nasa', 'edu', 'gov', 'launch', 'earth', 'moon', 'orbit', 'access', 'article']\n",
      "['god', 'edu', 'people', 'don', 'think', 'atheists', 'keith', 'does', 'atheism', 'believe']\n",
      "['edu', 'com', 'writes', 'article', 'posting', 'university', 'nntp', 'host', 'just', 'don']\n"
     ]
    }
   ],
   "source": [
    "top_n = 10 #Set the number of top words to select\n",
    "topics_words = [] #Create the list to hold the list of words\n",
    "\n",
    "#For each topic list, extract the list of words they represent\n",
    "for topic in fit_model.components_:\n",
    "    words = list(zip(topic, terms)) #Zip together the topic and term lists to connect commonality to the words\n",
    "    words.sort(reverse = True) #Sort the list by highest topic value, and thus the biggest words for said topic\n",
    "    top = words[:top_n] #Take the top n words from this topic set\n",
    "    top_words = [tup[1] for tup in top] #Pull out only the words from this\n",
    "    \n",
    "    print(top_words) #Print the common word list where I can see them\n",
    "    topics_words.append(top_words) #Add the list to the list holder for the lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a35f879-1a38-4c70-9b9f-fc999d0f36e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c38f970bc1c97621f7c37d8796bffdc",
     "grade": true,
     "grade_id": "cell-42928f50f2892932",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 15 pts\n",
    "def test_lda_model():\n",
    "    assert 'lda_model' in globals(), \"LDA model not defined\"\n",
    "    assert isinstance(lda_model, LatentDirichletAllocation), \"lda_model is not an instance of LatentDirichletAllocation\"\n",
    "    assert lda_model.n_components == 5, \"LDA model should have exactly 5 topics\"\n",
    "\n",
    "def test_topic_words():\n",
    "    assert 'topics_words' in globals(), \"topics_words not defined\"\n",
    "    assert isinstance(topics_words, list), \"topics_words should be a list\"\n",
    "    assert all(isinstance(topic, list) for topic in topics_words), \"Each topic in topics_words should be a list\"\n",
    "    assert all(len(topic) == 10 for topic in topics_words), \"Each topic should contain exactly 10 words\"\n",
    "\n",
    "# Run the tests\n",
    "test_lda_model()\n",
    "test_topic_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d948-53ca-4ace-9992-bdeb453717d8",
   "metadata": {},
   "source": [
    "**Q. 3.2** (5 pts) What do each of the topics represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631dd00-be2b-431b-b7fa-310fffa2af40",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1afaf16b215ccf822fd42de3982b730",
     "grade": true,
     "grade_id": "cell-04be3755268a6783",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The easy topics:\n",
    "\n",
    "Topic 1: This is the graphics topic. It is the only topic with software, image, graphics, and file.\n",
    "\n",
    "Topic 2: This topic and topic 4 are religious topics. It is hard to tell which due to overlap, so this one will be explained as the positive religious section since it contains more positive words.\n",
    "\n",
    "Topic 3: This is the space topic. It includes various space words like nasa, earth, and moon.\n",
    "\n",
    "Topic 4: This is another religious topic, but it holds more critical words and possibly questions regarding religion/atheism. It is hard to tell if this topic represents questions about atheism or more general religious topics given that god is the biggest keyword, so it will be called the critical religious section.\n",
    "\n",
    "The hard topic:\n",
    "\n",
    "This one is more difficult due to the first question having everything but 4 topics removed. Not only that, but two of the original topics overlap heavily due to both being related to religion (being atheism and religion; even if you personally believe atheism is not a religion, topics about it use enough religious terminology to have it lumped with religion in the eyes of a model looking at key words). I have added a random state of 42 and that splits the religion category up better. This leaves one topic in this case (two in most other random states) that does not fit the expected topics.\n",
    "\n",
    "Topics 5: This topic is the leftovers that are likely just common words from the forum. In most other states I looked at, this was represented by two nearly-identical topics. The 42 random state specifically does a better job capturing a difference in the two religious topics, which is why it is being used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
