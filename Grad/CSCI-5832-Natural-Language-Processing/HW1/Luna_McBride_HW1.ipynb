{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfsuonrWtD-3"
   },
   "source": [
    "# Introduction to Natural Language Processing: A Journey through Next Word Prediction\n",
    "\n",
    "Welcome to the fascinating realm of Natural Language Processing (NLP), a field where language, technology, and human cognition intersect. Here, we embark on a journey to explore one of the most intriguing aspects of NLP: next word prediction. This objective is not just a cornerstone in understanding NLP, but also a window into the intricate ways machines can learn to interpret and generate human language.\n",
    "\n",
    "Every day, we interact with technology that anticipates what weâ€™re going to say next - be it while typing an email, texting, or searching online. This seemingly magical ability of machines to predict the next word in a sentence is a product of NLP. But how does it work? The answer lies in the ability of algorithms to learn from vast amounts of text data and understand patterns in language use.\n",
    "\n",
    "Next word prediction is more than a tool for faster typing. It's a fundamental technology in a variety of applications - from enhancing accessibility in communication aids to powering chatbots and virtual assistants. Its implications span various domains, including accessibility, automation, and artificial intelligence, making it a crucial subject in the study of NLP.\n",
    "\n",
    "At the heart of next word prediction lies the challenge of dealing with the complexity and variability of language. Here, we introduce a probabilistic framework to think about next word prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgQjpoTJlha6"
   },
   "source": [
    "## Quick Probability Review\n",
    "Probability is a mathematical framework for quantifying uncertainty. It is a way of thinking about the likelihood of events. In the context of NLP, we can use probability to think about the likelihood of a word occurring in a sentence. For example, the probability of the word \"cat\" occurring in the sentence \"the cat is on the mat\" is 1/5, or 0.2.\n",
    "\n",
    "## Conditional Probability\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. For example, the probability of the word \"cat\" occurring immediately after the word \"the\" is 1/2, or 0.5. \n",
    "\n",
    "$P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "We can write this as P(cat|the) = 0.5.\n",
    "\n",
    "Why? Because there are two words that can follow \"the\" in this sentence: \"cat\" and \"mat\". Since there is only one \"cat\" in the sentence, the probability of \"cat\" occurring after \"the\" is 1/2.\n",
    "\n",
    "## Joint Probability\n",
    "Joint probability is the probability of two events occurring together. For example, the probability of the word \"cat\" occurring in the sentence \"the cat is on the mat\" is 1/5, or 0.2. The probability of the word \"mat\" occurring in the same sentence is 1/5, or 0.2. The joint probability of \"cat\" and \"mat\" occurring together in the sentence is 1/25, or 0.04.\n",
    "\n",
    "$\\mathbb{P}(A \\cap B)=\\mathbb{P}(B \\mid A) \\mathbb{P}(A)=\\frac{1}{5} \\cdot \\frac{1}{5}=\\frac{1}{25}$\n",
    "\n",
    "## Chain Rule of Probability\n",
    "The chain rule of probability is a formula for calculating the joint probability of multiple events. For example, the probability of the words \"cat\" and \"mat\" occurring together in the sentence \"the cat is on the mat\" is 1/25, or 0.04. The probability of the word \"the\" occurring in the same sentence is 2/5, or 0.4. The chain rule of probability states that the joint probability of \"cat\", \"mat\", and \"the\" occurring together in the sentence is equal to the product of the conditional probabilities of each word given the words that came before it. In this case, the joint probability is equal to 0.04 * 0.4 = 0.016.\n",
    "\n",
    "$\\begin{aligned}\n",
    "P\\left(X_1 \\ldots X_n\\right) & =P\\left(X_1\\right) P\\left(X_2 \\mid X_1\\right) P\\left(X_3 \\mid X_{1: 2}\\right) \\ldots P\\left(X_n \\mid X_{1: n-1}\\right) \\\\\n",
    "& =\\prod_{k=1}^n P\\left(X_k \\mid X_{1: k-1}\\right)\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCIfBKJbnUO5"
   },
   "source": [
    "## Building a Next Word Predictor\n",
    "\n",
    "<b>Let's start with a simple probability exercise. Consider the corpus shown below. It consists of 3 sentences contained in a list. Based purely on counting, write a function to compute the probability of any given word in the corpus. This should be a global probability, i.e. the probability of a word occurring anywhere in the corpus.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: these solutions have been run through again to minimize loops, as previous solutions had too much nesting. Normally I may prefer to use Numpy vectorization, but Numpy was not a given. As such, it became a challenge of limiting loop nesting only using base Python functions. This source was used due to its clear list of built-in, list, string, and dictionary functions:\n",
    "\n",
    "https://www.w3schools.com/python/python_ref_functions.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YTkz71lssV0A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "corpus = ['this is sentence one',\n",
    "          'this is sentence two',\n",
    "          'this is sentence three']\n",
    "\n",
    "#prob_sentences: returns the probability based on each sentence\n",
    "#Input: the sentence, the word being compared\n",
    "#Output: the probability in this sentence\n",
    "def prob_sentences(sentence, word):\n",
    "    prob_word = 1/len(corpus) #Get 1 over the number of sentences in the corpus, as this is one sentence out of the corpus\n",
    "    split_sentence = sentence.split() #Split the sentence into words\n",
    "    prob_in_sentence = split_sentence.count(word) #Count the number of occurances of the word\n",
    "    return (prob_in_sentence*prob_word)/len(split_sentence) #Return the occurances in the sentence times the probability of the sentence over the number of words in this sentence, or p(b|a)p(a)\n",
    "\n",
    "#prob: get the probability of the word for the corpus and return it\n",
    "#Input: the word, the corpus\n",
    "#Output: the probability\n",
    "def prob(word, corpus):\n",
    "    # your code here\n",
    "    probs = list(map(prob_sentences, corpus, [word]*len(corpus))) #Get the pribabilities by mapping the prob_sentences function over the corpus, thus returning the result for each sentence\n",
    "    return sum(probs) #Return the sum of the list of probabilites derived from each sentence to come to a single, final result.\n",
    "    \n",
    "print(prob(\"this\", corpus))\n",
    "print(prob(\"one\", corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_prob():\n",
    "    assert prob('this', corpus) == 0.25\n",
    "    assert prob('is', corpus) == 0.25\n",
    "    assert prob('sentence', corpus) == 0.25\n",
    "    assert round(prob('one', corpus),2) == 0.08\n",
    "    assert round(prob('two', corpus), 2) == 0.08\n",
    "    assert round(prob('three', corpus), 2) == 0.08\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_prob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one of the most important concepts in NLP is that of conditional probability. Conditional probability is the probability of an event occurring given that another event has already occurred. In the context of NLP, we can think of conditional probability as the probability of a word occurring given that another word has already occurred. For example, what is the probability of the word \"the\" occurring given that the word \"quick\" has already occurred? We can write this as P(the|quick).\n",
    "\n",
    "<b> Start by writing a function to return all bigrams in the corpus. The function should take a corpus as input and return a list of bigrams. A bigram is a tuple of two words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" contains 8 bigrams or co-occuring words: (The, quick), (quick, brown), (brown, fox), (fox, jumps), (jumps, over), (over, the), (the, lazy), (lazy, dog).\n",
    "\n",
    "We will also add start and end tokens to each sentence in the corpus. This is a common practice in NLP and is useful for computing probabilities. The start token will be represented by the string \"s\" and the end token by \"/s\". </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'one'), ('one', '</s>'), ('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'two'), ('two', '</s>'), ('<s>', 'this'), ('this', 'is'), ('is', 'sentence'), ('sentence', 'three'), ('three', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CORPUS #################\n",
    "corpus = ['<s> this is sentence one </s>',\n",
    "          '<s> this is sentence two </s>',\n",
    "          '<s> this is sentence three </s>']\n",
    "############################################\n",
    "\n",
    "#sentence_bigrams: get the bigrams given a sentence\n",
    "#Input: the sentence\n",
    "#Output: the bigrams\n",
    "def sentence_bigrams(sentence):\n",
    "    bigrams = [] #Create a list for the bigrams\n",
    "    split_sentence = sentence.split() #Split the sentence so that it gives words instead of characters\n",
    "    length_sentence = len(split_sentence) #Get the length of the sentence\n",
    "    \n",
    "    #For each index 1 to the end, create a bigram tuple out of the previous word and the current word\n",
    "    for i in range(1, length_sentence):\n",
    "        bigrams.append((split_sentence[i-1], split_sentence[i])) #Create the bigrams\n",
    "    \n",
    "    return bigrams #Return the bigrams\n",
    "\n",
    "def find_bigrams(corpus) -> list:\n",
    "    # your code here\n",
    "    bigram_dict = {} #A dictionary to hold bigrams, specifically to count their commonality later\n",
    "    bigrams = [] #A list to hold the bigrams\n",
    "    \n",
    "    bigram_list = list(map(sentence_bigrams, corpus)) #Map the sentence_bigrams functions over the corpus to get a list of lists of all bigrams from each sentence\n",
    "    \n",
    "    #This structure may be a bit odd, but it is to account for base python not having a reduce function\n",
    "    \n",
    "    #For each bigram list in the bigram list list, extend a bigram array so these lists come together as a single longer list\n",
    "    for bigram in bigram_list:\n",
    "        bigrams.extend(bigram) #Extend the bigram list to make the bigram lists into one list\n",
    "    \n",
    "    #Ignore this section. I initially interpreted the assignment to mean all bigrams of a specific count rather\n",
    "    #    than all bigrams total. I am keeping this part though, as the dictionary assignment may be useful to me later.\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    #For each bigram tuple in the bigram list, create a count for it using the bigram dictionary\n",
    "    #for bigram_tuple in bigrams:\n",
    "        \n",
    "        #If the bigram is already in the dictionary, increase the count\n",
    "        #if bigram_tuple in bigram_dict:\n",
    "            #bigram_dict[bigram_tuple] += 1 #Increase the count for this bigram\n",
    "            \n",
    "        #If the bigram is not already in the list, add it to the list at a count of 1\n",
    "        #else:\n",
    "            #bigram_dict[bigram_tuple] = 1 #Add the bigram to the list\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "            \n",
    "    return bigrams #Return the bigrams\n",
    "\n",
    "print(find_bigrams(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_bigrams():\n",
    "    assert find_bigrams(corpus)[0] == ('<s>', 'this')\n",
    "    assert find_bigrams(corpus)[1] == ('this', 'is')\n",
    "    assert find_bigrams(corpus)[2] == ('is', 'sentence')\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the bigrams, we can compute the probability of any word occurring after another as follows:\n",
    "\n",
    "$P(\\text{word2}|\\text{word1}) = \\frac{Count(\\text{word1}, \\text{word2})}{Count(\\text{word1})}$, where $Count(\\text{word1}, \\text{word2})$ is the number of times $\\text{word1}$ and $\\text{word2}$ occur together and $Count(\\text{word1})$ is the number of times $\\text{word1}$ occurs.\n",
    "\n",
    "For a clearer idea of why this formula works, refer page 4, chapter 3 of Jurafsky and Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "<b>Write a function to compute the conditional probability of a word given another word. The function should take a corpus and two words as input and return the conditional probability of the second word given the first word. For example, the probability of the word \"the\" occurring given that the word \"quick\" has already occurred is P(the|quick).</b>\n",
    "\n",
    "<i>Hint: You can use the function you wrote earlier to get all bigrams in the corpus. You can also use the function you wrote earlier to compute the probability of any given word in the corpus.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = find_bigrams(corpus)\n",
    "\n",
    "def conditional_prob(word:str, prev_word:str, bigrams:list) -> float:\n",
    "    # your code here\n",
    "    num_prev_word = 0 #Holder for the number of bigrams with the previous word\n",
    "    num_word_and_prev = 0 #Holder for the number of bigrams with both the previous word and the current word\n",
    "    \n",
    "    \n",
    "    #For each bigram, see if it is prev_word. If so, open the gate. If the gate is open and the second word is word, that means it is the prev_word word bigram\n",
    "    for bigram in bigrams:\n",
    "        gate = 0 #Close the gate, the gate being used to eliminate nested if statements\n",
    "        \n",
    "        #If the first word is prev_word, add it to the number of prev_word tuples and open the gate\n",
    "        if bigram[0] == prev_word:\n",
    "            num_prev_word += 1 #Add this tuple to the number of prev_word tuples\n",
    "            gate = 1 #Open the gate\n",
    "            \n",
    "        #If the gate is open (if the first word of the tuple is prev_word) and the second word of the tuple is word, add that to the counter for the prev_word word bigrams\n",
    "        if gate == 1 and bigram[1] == word:\n",
    "            num_word_and_prev += 1 #Up the counter for the number of prev_word word bigrams\n",
    "    \n",
    "    return num_word_and_prev/num_prev_word #Return the conditional probability\n",
    "\n",
    "conditional_prob('one', 'sentence', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the code got a divide by 0 error!\n"
     ]
    }
   ],
   "source": [
    "#Change the cell to a try except cell to still show the divide by 0 while still letting me clear all outputs and rerun the cells\n",
    "try:\n",
    "    conditional_prob('one', 'this', corpus)\n",
    "except:\n",
    "    print(\"This version of the code got a divide by 0 error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good but you might be getting a division by zero error! If this is the case, let's use laplace smoothing to improve it. Laplace smoothing is a technique for dealing with words that do not occur in the corpus. It is a way of adjusting the probability of a word occurring given another word by adding 1 to the numerator and the number of unique words in the corpus to the denominator. This ensures that the probability of a word occurring is never 0. For a clearer idea of why this formula works, refer page 6, chapter 3 of Jurafsky and Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "To implement laplace smoothing, we need to modify our conditional probability function as follows:\n",
    "\n",
    "$P\\left(\\text{word2}|\\text{word1} \\right) = \\frac{Count(\\text{word1},\\text{word2})+1}{Count(\\text{word1})+V}$ \n",
    "\n",
    "where $Count(\\text{word1},\\text{word2})$ is the number of times $\\text{word1}$ and $\\text{word2}$ occur together, $Count(\\text{word1})$, is the number of times $\\text{word1}$ occurs, and $V$ is the number of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = find_bigrams(corpus)\n",
    "\n",
    "def conditional_prob(word:str, prev_word:str, bigrams:list, k = 1) -> float:\n",
    "    # k is the smoothing constant and is set to 1 by default for laplace smoothing\n",
    "    # your code here\n",
    "    num_prev_word = 0 #Holder for the number of bigrams with the previous word\n",
    "    num_word_and_prev = 0 #Holder for the number of bigrams with both the previous word and the current word\n",
    "    \n",
    "    #Multiple ideas were tried for the unique section, but this was the fastest. list(set(bigrams, ())) calculated perplexity\n",
    "    #  176 seconds, while [word for sublist in bigrams for word in sublist] took 24 seconds and this took 0.05 seconds\n",
    "    \n",
    "    unique = list(dict(bigrams).keys()) #Convert the bigrams into dictionary, then take a list of their keys\n",
    "    unique.append(\"</s>\") #Append the end tag, as that is the one item that is in the values but not the keys\n",
    "    \n",
    "    #For each bigram, see if it is prev_word. If so, open the gate. If the gate is open and the second word is word, that means it is the prev_word word bigram\n",
    "    for bigram in bigrams:\n",
    "        gate = 0 #Close the gate, the gate being used to eliminate nested if statements\n",
    "        \n",
    "        #If the first word is prev_word, add it to the number of prev_word tuples and open the gate\n",
    "        if bigram[0] == prev_word:\n",
    "            num_prev_word += 1 #Add this tuple to the number of prev_word tuples\n",
    "            gate = 1 #Open the gate\n",
    "            \n",
    "        #If the gate is open (if the first word of the tuple is prev_word) and the second word of the tuple is word, add that to the counter for the prev_word word bigrams\n",
    "        if gate == 1 and bigram[1] == word:\n",
    "            num_word_and_prev += 1 #Up the counter for the number of prev_word word bigrams\n",
    "    \n",
    "    \n",
    "    return (num_word_and_prev+1)/(num_prev_word+len(unique)) #Return the conditional probability\n",
    "\n",
    "conditional_prob('sentence', 'this', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18181818181818182"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_prob('one', 'sentence', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_conditional_prob():\n",
    "    assert round(conditional_prob('is', 'this', bigrams), 2) == 0.36\n",
    "    assert round(conditional_prob('sentence', 'this', bigrams), 2) == 0.09\n",
    "    assert round(conditional_prob('one', 'sentence', bigrams),2) == 0.18\n",
    "    assert round(conditional_prob('two', 'sentence', bigrams), 2) == 0.18\n",
    "    assert round(conditional_prob('three', 'sentence', bigrams), 2) == 0.18\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "\n",
    "test_conditional_prob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and predict the next word in a sentence. This is essentially what an advanced chatbot like ChatGPT also tries to do, although ChatGPT uses a much bigger corpus and a more sophisticated algorithm. We will use the same corpus as before, but this time we will use the conditional probability formula to predict the next word in a sentence. We will also use the start and end tokens to help us compute the probabilities.\n",
    "\n",
    "<b>Write a function to predict the next word in a sentence. The function should take a corpus and a sentence as input and return the most likely next word in the sentence. For example, if the sentence is \"the quick brown\", the function should return \"fox\".</b>\n",
    "\n",
    "<i>Hint: You can use the conditional probability function you wrote earlier to compute the probability of each word in the vocabulary occurring after the last word in the sentence. The word with the highest probability is the most likely next word. In other words, check the probability of each possible word in the corpus with respect to the given previous word and pick the one with the maximum probability. To make your algorithm more efficient, you can ignore non-co-occuring pairs when you build the vocabulary. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_next_word(prev_word: str, bigrams: list) -> str:\n",
    "    # your code here\n",
    "    prob_dict = {} #Create a dictionary to put the possible words with their probabilities\n",
    "    highest_prob = 0 #Set the highest probability found so far for comparison\n",
    "    highest_prob_word = \"\" #Set the highest probability word thus far to go with the highest probability\n",
    "    \n",
    "    #For each bigram, find the possible words based on the prev_word and their probability, then put them in a dictionary\n",
    "    for bigram in bigrams:\n",
    "        \n",
    "        #If the first word in the pair is the prev_word, get the probability of the second word being correct and put it in the dictionary\n",
    "        if bigram[0] == prev_word:\n",
    "            prob_dict[bigram[1]] = conditional_prob(bigram[1], prev_word, bigrams) #Put the word and probability in the dictionary\n",
    "    \n",
    "    #For each word probability pair in the dictionary, see if it has the highest probability. If so, set it as such\n",
    "    for word, prob in prob_dict.items():\n",
    "        \n",
    "        #See if this probability is higher than the current highest probability\n",
    "        if prob > highest_prob:\n",
    "            highest_prob = prob #Set the new highest probability\n",
    "            highest_prob_word = word #Set the new highest probability word\n",
    "            \n",
    "    return highest_prob_word #Return the word with the highest probability\n",
    "\n",
    "predict_next_word('this', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word('sentence', bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_predict_next_word():\n",
    "    assert predict_next_word('this', bigrams) == 'is'\n",
    "    assert predict_next_word('is', bigrams) == 'sentence'\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_predict_next_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and predict an entire sentence now.\n",
    "\n",
    "<b>Write a function that takes an initial string and a word limit as input and returns a sentence of the specified length. The initial string is the starting point for the sentence. For example, if the initial string is \"the quick brown\" and the word limit is 5, the function should return \"the quick brown fox jumps over\". The function should use the predict_next_word function you wrote earlier to predict the next word in the sentence.</b>\n",
    "\n",
    "<i>Hint: You can use a for loop to predict the next word in the sentence. The initial string is the starting point for the sentence. At each iteration, you can add the predicted word to the sentence and use the predicted word as the new initial string. You can stop the loop when the word limit is reached. </i>\n",
    "\n",
    "<i>Hint: You can use try, except to handle the case where the initial string is not in the corpus. If you don't catch the exception, you might encounter a value error. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(input_sentence, bigrams, limit=2):\n",
    "    # your code here\n",
    "    count = 0 #Create a count variable so that the limit is not breached\n",
    "    sentence = input_sentence #Create a sentence variable to hold the sentence as it changes\n",
    "    \n",
    "    #While not at the limit, generate next words. A while loop is used instead of a for loop to allow for more intuitive breaking\n",
    "    while count < limit:\n",
    "        split_sentence = sentence.split() #Split the sentence so that indexing gives a whole word rather than a letter\n",
    "        next_word = predict_next_word(split_sentence[-1], bigrams) #Predict the next word based on the previous word\n",
    "        sentence = f\"{sentence} {next_word}\" #Add the new word to the sentence, using an fstring so that all of the +s and extra spaces are not required\n",
    "        count += 1 #Increase the counter\n",
    "        \n",
    "        #If the next word is the ending, break from the loop\n",
    "        if next_word == \"</s>\":\n",
    "            break #Break from the loop\n",
    "        \n",
    "    return sentence #Return the generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is sentence one </s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"This is\", bigrams, limit = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_predict_sentence():\n",
    "    assert predict_sentence(\"This is\", bigrams, limit = 7) == 'This is sentence one </s>'\n",
    "    assert predict_sentence(\"This is\", bigrams, limit = 1) == 'This is sentence'\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "\n",
    "test_predict_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try this with a real corpus. We will use the Brown corpus, which is a collection of 500 samples of English-language text, totaling roughly one million words. The Brown corpus is a good choice for this exercise because it is a balanced corpus, meaning it contains text from a variety of genres. This makes it a good representation of the English language. The Brown corpus is also a tagged corpus, meaning it contains information about the part of speech of each word. We will use this information later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lunam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# import the brown corpus from nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import re\n",
    "\n",
    "# download the corpus\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . </s>\", \"<s> The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . </s>\"]\n"
     ]
    }
   ],
   "source": [
    "# get the corpus\n",
    "corpus_brown = brown.sents()\n",
    "\n",
    "corpus_brown = [' '.join(sentence) for sentence in corpus_brown]\n",
    "# Add start and end tags\n",
    "corpus_brown = ['<s> ' + sentence + ' </s>' for sentence in corpus_brown][:3000]\n",
    "print(corpus_brown[0:2])\n",
    "bigrams_brown = find_bigrams(corpus_brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and predict a series of next words using the Brown corpus. The operation below might take a few seconds to run based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is a year . </s>\n",
      "Time taken: 2.27727267742157 minutes\n"
     ]
    }
   ],
   "source": [
    "import time #Allowing this import to show how slow my system can be and, thus, why I originally wanted to go for vectorization\n",
    "\n",
    "start = time.time()\n",
    "print(f\"Sentence: {predict_sentence('This', bigrams_brown, limit = 20)}\")\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken: {(end - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version with the list(dict.keys) in the conditional probability may show this time, but other ideas that were tried took several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: <s> The President Kennedy administration . </s>\n",
      "Sentence: <s> The President Kennedy administration . </s>\n",
      "Sentence: <s> The President Kennedy administration . </s>\n",
      "Sentence: <s> The President Kennedy administration . </s>\n",
      "Sentence: <s> The President Kennedy administration . </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Sentence: {predict_sentence('<s>', bigrams_brown, limit = 20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A primer on perplexity\n",
    "Perplexity is a measurement in Natural Language Processing (NLP) used to evaluate language models. It's a measure of how well a probability model predicts a sample. A lower perplexity score indicates better performance of the model.\n",
    "\n",
    "The perplexity of a sentence is calculated as the inverse probability of the sentence, normalized by the number of words. In other words, it's the geometric mean of the inverse conditional probability of each word given the previous word in the sentence.\n",
    "\n",
    "$\\text{Perplexity}(W)=\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\ldots, w_N)}}=\\sqrt[N]{\\prod_{i=1}^N \\frac{1}{P(w_i \\mid w_1, \\ldots, w_{i-1})}}$\n",
    "\n",
    "where $N$ is the number of words in the sentence and $P(w_i \\mid w_1, \\ldots, w_{i-1})$ is the conditional probability of the i'th word given the previous words in the sentence.\n",
    "\n",
    "In our case, since we only have bigrams, the formula becomes:\n",
    "$\\text{Perplexity}(W)=\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\ldots, w_N)}}=\\sqrt[N]{\\prod_{i=1}^N \\frac{1}{P(w_i \\mid w_{i-1})}}$\n",
    "\n",
    "Perplexity is a useful metric for evaluating language models because it is a measure of how well a probability model predicts a sample. A lower perplexity score indicates better performance of the model.\n",
    "\n",
    "Let's try and calculate the perplexity for a few sentences in the Brown corpus. We will use the same corpus as before, but this time we will use the perplexity formula to calculate the perplexity of each sentence. We will also use the start and end tokens to help us compute the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8284271247461903"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity(sentence, bigrams) -> float:\n",
    "    # your code here\n",
    "    perplexity_denom = 1 #A holder variable for the probability, starting at 1 as 1*x = x\n",
    "    \n",
    "    new_sentence = sentence #Set the sentence. This is a remnant from when adding the tags was the expectation and I do not want to change everything to accommodate.\n",
    "    \n",
    "    split_sentence = new_sentence.split() #Split the sentence for better counting\n",
    "    len_sentence = len(split_sentence) #Get the number of words\n",
    "    prev_word = \"\" #Set a previous word variable so (nothing, <s>) is not included\n",
    "    \n",
    "    #For each word, get its probability with the previous word. If there is no previous word, set this word to the previous word\n",
    "    for word in split_sentence:\n",
    "        #If there is a previous word, compute the probability\n",
    "        if prev_word != \"\":\n",
    "            perplexity_denom *= conditional_prob(word, prev_word, bigrams) #Multiply the probability by the base probability\n",
    "        prev_word = word #Set the current word to the previous word\n",
    "        \n",
    "    return (1/perplexity_denom)**(1/len_sentence) #Compute the perplexity (1/probability)**(1/number of words) and return it\n",
    "    \n",
    "perplexity(\"This is\", bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your solution is working. Good job!\n"
     ]
    }
   ],
   "source": [
    "# Test the function - Do not change below this line\n",
    "def test_perplexity():\n",
    "    corpus = ['<s> this is sentence one </s>',\n",
    "              '<s> this is sentence two </s>',\n",
    "              '<s> this is sentence three </s>']\n",
    "    bigrams = find_bigrams(corpus)\n",
    "    perplexity_score = perplexity(\"This is \", bigrams)\n",
    "    assert round(perplexity_score, 2) == 2.83\n",
    "    print(\"Your solution is working. Good job!\")\n",
    "    \n",
    "test_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pick 5 sentences from the Brown corpus and calculate the perplexity of each sentence. The code to import and load the corpus is provided above. Also calculate the average perplexity of the subset of the brown corpus chosen above.\n",
    "\n",
    "You can also make up your own sentences and calculate the perplexity of each sentence. How does the perplexity of your sentences compare to the perplexity of the sentences in the Brown corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.06854438781738281 Seconds\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "#Timing taken for optimization purposes. The perplexity only uses conditional probability, so this was best to make changes.\n",
    "start = time.time()\n",
    "perplexity(\"This is \", bigrams_brown)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken: {end-start} Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Number: 1255\n",
      "Sentence: <s> It was Gardner's second run batted in of the game and his only ones of the year . </s>\n",
      "Perplexity: 666.7970999406034\n",
      "---------------------------------------------------\n",
      "Sentence Number: 630\n",
      "Sentence: <s> The Democratic leadership , however , hopes to pass it sometime this week . </s>\n",
      "Perplexity: 748.7320729621073\n",
      "---------------------------------------------------\n",
      "Sentence Number: 191\n",
      "Sentence: <s> These , he said , are `` two of the principal underlying causes for family breakups leading to ADC '' . </s>\n",
      "Perplexity: 928.2808278556488\n",
      "---------------------------------------------------\n",
      "Sentence Number: 2195\n",
      "Sentence: <s> He indicated that requests would be made for more U.S. arms and more U.S. military advisers . </s>\n",
      "Perplexity: 1237.8332490003231\n",
      "---------------------------------------------------\n",
      "Sentence Number: 1263\n",
      "Sentence: <s> -- Boston Red Sox Outfielder Jackie Jensen said Monday night he was through playing baseball . </s>\n",
      "Perplexity: 1402.044739353111\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Pseudo-random number generation ideas discussed here since I could not add random.random\n",
    "# https://stackoverflow.com/questions/11946622/implementation-of-random-number-generator\n",
    "brown_corpus_length = len(corpus_brown) #Get the length of the corpus\n",
    "prev_factor = 42 #Choose a prev factor\n",
    "change_factor = 24 #Choose a change factor for the equation\n",
    "\n",
    "#Pseudo-randomly choose 5 sentences and find their perplexity\n",
    "for i in range(1, 6):\n",
    "    change_factor = (change_factor**i * prev_factor**i + len(corpus_brown[i]))%brown_corpus_length #Pseudo-randomly choose a sentence from the corpus\n",
    "        \n",
    "    prev_factor = change_factor #Update the prev factor\n",
    "    change_factor = change_factor - i #Update the change factor\n",
    "    sentence = corpus_brown[change_factor] #Pull the selected sentence\n",
    "    \n",
    "    #Print information about the random selection\n",
    "    print(f\"Sentence Number: {change_factor}\")\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Perplexity: {perplexity(sentence, bigrams_brown)}\")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.60501918343532\n",
      "5454.223720118167\n"
     ]
    }
   ],
   "source": [
    "print(perplexity(\"short sentence\", bigrams_brown))\n",
    "print(perplexity(\"\"\"This is a long sentence: \n",
    "According to all known laws of aviation, \n",
    "there is no way a bee should be able to fly. \n",
    "Its wings are too small to get its fat little body off the ground.\n",
    "The bee, of course, flies anyway because bees don't care what humans think is impossible.\"\"\", bigrams_brown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full Bee Movie script (found here: https://courses.cs.washington.edu/courses/cse163/20wi/files/lectures/L04/bee-movie.txt) caused a divide by 0 error on the perplexity, but the intro does just fine. It is interesting, though, that the Bee Movie script intro only has a perplexity of 5282 when some of the ones from the original corpus near 2000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a brief half-page report on the concepts you explored in this exercise. You can use the questions below as a guide.\n",
    "1) What is the conditional probability of a word occurring given its previous word and how does this probability change when you use laplace smoothing?\n",
    "2) What do you expect will happen when you use trigrams instead of bigrams to predict the next word in a sentence? Does the perplexity increase or decrease? Why?\n",
    "3) What do you expect will happen when you use a larger corpus to predict the next word in a sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! You've built yourself a next word predictor. Now go put it out there and get acquired by Microsoft for a few billion dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further exercises (will not be graded):\n",
    "\n",
    "- Try to improve the model by using trigrams instead of bigrams. A trigram is a tuple of three words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" contains 7 trigrams: (The, quick, brown), (quick, brown, fox), (brown, fox, jumps), (fox, jumps, over), (jumps, over, the), (over, the, lazy), (the, lazy, dog). You can use the same formula to compute the conditional probability of a word given two other words. For example, the probability of the word \"the\" occurring given that the words \"quick\" and \"brown\" have already occurred is P(the|quick, brown). You can also use the start and end tokens to help you compute the probabilities. For example, the probability of the word \"the\" occurring given that the words \"s\" and \"quick\" have already occurred is P(the|s, quick). You can use the same function you wrote earlier to compute the conditional probability of a word given two other words. The function should take a corpus and three words as input and return the conditional probability of the third word given the first two words. For example, the probability of the word \"the\" occurring given that the words \"quick\" and \"brown\" have already occurred is P(the|quick, brown)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
